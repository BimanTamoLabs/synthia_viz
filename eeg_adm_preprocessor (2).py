# -*- coding: utf-8 -*-
"""EEG_ADM_preprocessor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ptdImjKkZ7SaKSGG3UwntGk4UIvGHpny
"""

from google.colab import drive
drive.mount('/content/drive')

import os

def find_files(main_folder_location, patient_id_folder=None, file_type='EEG'):
    """
    Finds the full paths of files containing a specified keyword and extension
    within a specific patient ID subfolder or within all patient ID subfolders
    if none is specified.

    Args:
        main_folder_location (str): The path to the main folder.
        patient_id_folder (str, optional): The name of a specific patient ID subfolder.
                                           If None, searches all subfolders.
        file_type (str, optional): The type of file to search for ('EEG' for CSV, 'HRV' for JSON).
                                   Defaults to 'EEG'.

    Returns:
        list: A list of full paths to the found files.
    """
    found_files = []
    if not os.path.isdir(main_folder_location):
        print(f"Error: Main folder not found at {main_folder_location}")
        return found_files

    if file_type == 'EEG':
        keyword = 'EEG'
        extension = '.csv'
    elif file_type == 'HRV':
        keyword = 'HRV'
        extension = '.json'
    else:
        print(f"Error: Invalid file_type specified: {file_type}. Choose 'EEG' or 'HRV'.")
        return found_files

    if patient_id_folder:
        patient_folder_path = os.path.join(main_folder_location, patient_id_folder)
        if os.path.isdir(patient_folder_path):
            for file_name in os.listdir(patient_folder_path):
                if file_name.endswith(extension) and keyword in file_name:
                    found_files.append(os.path.join(patient_folder_path, file_name))
        else:
            print(f"Error: Patient ID folder not found at {patient_folder_path}")
    else:
        for patient_id_folder in os.listdir(main_folder_location):
            patient_folder_path = os.path.join(main_folder_location, patient_id_folder)
            if os.path.isdir(patient_folder_path):
                for file_name in os.listdir(patient_folder_path):
                    if file_name.endswith(extension) and keyword in file_name:
                        found_files.append(os.path.join(patient_folder_path, file_name))

    return found_files

main_folder = "/content/drive/MyDrive/TamoTech/Synthia/23_10_25"
patient_id = "5900123102510"

file_type = 'EEG'

eeg_file = find_files(main_folder, patient_id, file_type=file_type)

if eeg_file:
    print(f"Found EEG CSV files for patient ID {patient_id}:")
    for file_path in eeg_file:
        print(file_path)
else:
    print(f"No EEG CSV files found for patient ID {patient_id} in {main_folder}")

file_type = 'HRV'

hrv_file = find_files(main_folder, patient_id, file_type=file_type)

if hrv_file:
    print(f"Found {file_type} files for patient ID {patient_id}:")
    for file_path in hrv_file:
        print(file_path)
else:
    print(f"No {file_type} files found for patient ID {patient_id} in {main_folder}")

import pandas as pd

def process_eeg_data(file_path):
    """
    Reads an EEG CSV file, renames the first column to 'Timestamp',
    converts 'Timestamp' to datetime with a specific format,
    extracts EEG values into an array, adds 'EEG_Count', 'EEG_Min',
    and 'EEG_Max' columns, handles potential errors and removes rows
    with NaN in percentage columns.

    Args:
        file_path (str): The path to the EEG CSV file.

    Returns:
        pd.DataFrame: The processed DataFrame with 'Timestamp' (as datetime objects),
                      'EEG_Values', 'EEG_Count', 'EEG_Min', and 'EEG_Max' columns,
                      with rows containing NaN in percentage columns removed, or None if an error occurred.
    """
    try:
        # Attempt to read the CSV, skipping blank lines and using default pandas engine
        df = pd.read_csv(file_path, skip_blank_lines=True, encoding='utf-8')

        # Rename the first column to 'Timestamp'
        df.rename(columns={df.columns[0]: 'Timestamp'}, inplace=True)

        # Convert 'Timestamp' column to datetime objects with the specified format
        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')

        # Extract EEG values into an array from the 'EEG' column onwards
        # Assuming 'EEG' is the starting point for the values to be combined
        eeg_start_index = df.columns.get_loc('EEG')
        df['EEG_Values'] = df.iloc[:, eeg_start_index:].values.tolist()

        # Add 'EEG_Count' column with the length of the EEG_Values array
        df['EEG_Count'] = df['EEG_Values'].apply(len)

        # Calculate and add 'EEG_Min' and 'EEG_Max' columns
        df['EEG_Min'] = df['EEG_Values'].apply(lambda x: min(x) if x else None)
        df['EEG_Max'] = df['EEG_Values'].apply(lambda x: max(x) if x else None)


        # Drop the original individual EEG columns if no longer needed
        df.drop(df.columns[eeg_start_index:-4], axis=1, inplace=True)

        # Drop rows where any of the percentage columns have NaN values
        percentage_columns = ['Delta(%)', 'Theta(%)', 'Beta(%)', 'Alpha(%)']
        df.dropna(subset=percentage_columns, inplace=True)


        return df

    except FileNotFoundError:
        print(f"Error: The file was not found at {file_path}")
        return None
    except Exception as e:
        print(f"An error occurred while reading or processing the CSV file: {e}")
        return None

eeg_file_path = "/content/drive/MyDrive/TamoTech/Synthia/23_10_25/5900123102510/5900123102510_EEG - Sheet.csv"

# Process the DataFrame using the defined function
processed_eeg_df = process_eeg_data(eeg_file_path)

# Display the first 5 rows of the processed DataFrame if processing was successful
if processed_eeg_df is not None:
    display(processed_eeg_df.head())

import pandas as pd

def get_column_statistics(df, exclude_columns=None):
    """
    Calculates descriptive statistics for specified columns in a DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.
        exclude_columns (list, optional): A list of column names to exclude from statistics.
                                           Defaults to None.

    Returns:
        pd.DataFrame: A DataFrame containing the descriptive statistics for the selected columns.
    """
    if exclude_columns:
        columns_to_include = [col for col in df.columns if col not in exclude_columns]
        stats_df = df[columns_to_include].describe()
    else:
        stats_df = df.describe()

    return stats_df

# Example usage with the processed_eeg_df from the previous step
# Let's exclude 'Timestamp' and 'EEG_Values' as they are not numerical columns for standard statistics
columns_to_exclude = ['Timestamp', 'EEG_Values']
eeg_statistics = get_column_statistics(processed_eeg_df, exclude_columns=columns_to_exclude)

# Display the calculated statistics
display(eeg_statistics)

processed_eeg_df.tail()

import pandas as pd
import pprint

def validate_eeg_data(df):
    """
    Validates the processed EEG DataFrame and provides key session details.

    Args:
        df (pd.DataFrame): The processed EEG DataFrame with a 'Timestamp' column.

    Returns:
        dict: A dictionary containing validation results, or None if input is invalid.
    """
    if df is None or not isinstance(df, pd.DataFrame) or 'Timestamp' not in df.columns:
        print("Error: Invalid DataFrame provided for validation.")
        return None

    validation_results = {}

    # Get starting and ending time
    if not df['Timestamp'].empty:
        validation_results['start_time'] = df['Timestamp'].min()
        validation_results['end_time'] = df['Timestamp'].max()

        # Calculate total time duration
        validation_results['total_duration'] = validation_results['end_time'] - validation_results['start_time']
    else:
        validation_results['start_time'] = None
        validation_results['end_time'] = None
        validation_results['total_duration'] = None


    # Get valid data points (number of rows)
    validation_results['valid_data_points'] = len(df)

    # Estimate sampling rate (assuming roughly uniform sampling)
    # This is an estimation and might not be accurate for non-uniform sampling
    if validation_results['total_duration'] and validation_results['valid_data_points'] > 1:
        # Convert total duration to seconds
        total_duration_seconds = validation_results['total_duration'].total_seconds()
        validation_results['estimated_sampling_rate_hz'] = (validation_results['valid_data_points'] - 1) / total_duration_seconds if total_duration_seconds > 0 else 0
    else:
        validation_results['estimated_sampling_rate_hz'] = None


    # Check missing values
    validation_results['missing_values_per_column'] = df.isnull().sum().to_dict()

    # Check for anomalies (simple example: negative values in percentage columns)
    percentage_columns = ['Delta(%)', 'Theta(%)', 'Beta(%)', 'Alpha(%)']
    anomalies = {}
    for col in percentage_columns:
        if col in df.columns:
            negative_values = df[df[col] < 0].shape[0]
            if negative_values > 0:
                anomalies[f'negative_values_in_{col}'] = negative_values
    validation_results['anomalies'] = anomalies


    # Verify brain wave percentages sum is approximately 100%
    # Consider a small tolerance for floating point inaccuracies
    if all(col in df.columns for col in percentage_columns):
        df['percentage_sum'] = df[percentage_columns].sum(axis=1)
        # Check if the sum is close to 100 (e.g., within 1%)
        validation_results['percentage_sum_check_failed_rows'] = df[~df['percentage_sum'].between(99, 101)].shape[0]
        # Drop the temporary percentage_sum column
        df.drop('percentage_sum', axis=1, inplace=True)
    else:
        validation_results['percentage_sum_check_failed_rows'] = 'N/A (Percentage columns not found)'


    return validation_results

# Example usage with the processed_eeg_df
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    eeg_validation_results = validate_eeg_data(processed_eeg_df)
    if eeg_validation_results:
        print("EEG Data Validation Results:")
        pprint.pprint(eeg_validation_results)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

def plot_consciousness_metrics(df, weights):
    """
    Plots time series subplots for CSI, EMG, SQI, BS, and a combined Consciousness Score
    with specific titles, colors, and formatting, sets x-axis ticks to 2-minute intervals,
    formats timestamps to HH:MM:SS, and adds a main title with the session date.

    Args:
        df (pd.DataFrame): The input DataFrame with 'Timestamp', 'CSI', 'EMG', 'SQI', and 'BS' columns.
        weights (dict): A dictionary with weights for 'CSI', 'EMG', 'SQI', and 'BS' to calculate the Consciousness Score.
                        The sum of the weights must be 1.
    """
    if not all(col in df.columns for col in ['Timestamp', 'CSI', 'EMG', 'SQI', 'BS']):
        print("Error: DataFrame must contain 'Timestamp', 'CSI', 'EMG', 'SQI', and 'BS' columns.")
        return
    if not isinstance(weights, dict) or sum(weights.values()) != 1 or not all(key in weights for key in ['CSI', 'EMG', 'SQI', 'BS']):
         print("Error: Weights must be a dictionary with keys 'CSI', 'EMG', 'SQI', and 'BS', and the sum of values must be 1.")
         return

    # Calculate the Consciousness Score
    df['Consciousness Score'] = (
        df['CSI'] * weights.get('CSI', 0) +
        df['EMG'] * weights.get('EMG', 0) +
        df['SQI'] * weights.get('SQI', 0) +
        df['BS'] * weights.get('BS', 0)
    )

    metrics_to_plot = ['CSI', 'EMG', 'SQI', 'BS', 'Consciousness Score']
    subplot_titles = {
        'CSI': 'CSI - Consciousness State Index',
        'EMG': 'EMG - Electromyography Muscle Activity',
        'SQI': 'SQI - Signal Quality Index',
        'BS': 'BS - Brain State',
        'Consciousness Score': f"Composite Consciousness Score = {weights.get('CSI', 0)}*CSI + {weights.get('EMG', 0)}*EMG + {weights.get('SQI', 0)}*SQI + {weights.get('BS', 0)}*BS"
    }
    y_axis_labels = {
        'CSI': 'CSI Value',
        'EMG': 'EMG Value',
        'SQI': 'SQI Value',
        'BS': 'BS Value',
        'Consciousness Score': 'Composite Score'
    }
    colors = ['blue', 'green', 'red', 'purple', 'orange'] # Define different colors for each subplot

    fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=1, sharex=True, figsize=(12, 3 * len(metrics_to_plot)))

    # Set x-axis major locator to 2-minute intervals
    major_locator = mdates.MinuteLocator(interval=2)
    # Set x-axis formatter to show only HH:MM:SS
    major_formatter = mdates.DateFormatter('%H:%M:%S')

    fig.autofmt_xdate() # Auto-format date labels

    for i, metric in enumerate(metrics_to_plot):
        axes[i].plot(df['Timestamp'], df[metric], color=colors[i])
        axes[i].set_ylabel(y_axis_labels[metric])
        axes[i].set_title(subplot_titles[metric]) # Set individual subplot titles
        axes[i].grid(True)
        axes[i].xaxis.set_major_locator(major_locator) # Apply the locator to each subplot
        axes[i].xaxis.set_major_formatter(major_formatter) # Apply the formatter to each subplot


    axes[-1].set_xlabel("Time") # Set common x-axis label at the bottom

    # Get the date from the first timestamp
    session_date = df['Timestamp'].iloc[0].strftime('%Y-%m-%d')
    main_title = f"EEG Monitoring - Main Metrics Over Time\nTotal session on {session_date}"
    fig.suptitle(main_title, y=1.02) # Overall title

    plt.tight_layout()
    plt.show()

# Example Usage (assuming processed_eeg_df is available)
# You need to define the weights based on your domain knowledge or requirements
consciousness_weights = {'CSI': 0.6, 'EMG': 0.3, 'SQI': 0.1, 'BS': 0.0} # Example weights, sum must be 1

if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    plot_consciousness_metrics(processed_eeg_df, consciousness_weights)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

def plot_brain_wave_frequencies(df):
    """
    Plots time series subplots for Delta, Theta, Beta, and Alpha brain wave percentages
    with appropriate titles and formatting, sets x-axis ticks to 2-minute intervals,
    and fills the area under each curve.

    Args:
        df (pd.DataFrame): The input DataFrame with 'Timestamp', 'Delta(%)', 'Theta(%)',
                           'Beta(%)', and 'Alpha(%)' columns.
    """
    percentage_columns = ['Delta(%)', 'Theta(%)', 'Beta(%)', 'Alpha(%)']
    if not all(col in df.columns for col in ['Timestamp'] + percentage_columns):
        print("Error: DataFrame must contain 'Timestamp' and the following columns:", percentage_columns)
        return

    subplot_titles = {
        'Delta(%)': 'Delta (%)',
        'Theta(%)': 'Theta (%)',
        'Beta(%)': 'Beta (%)',
        'Alpha(%)': 'Alpha (%)'
    }
    y_axis_labels = {
        'Delta(%)': 'Percentage',
        'Theta(%)': 'Percentage',
        'Beta(%)': 'Percentage',
        'Alpha(%)': 'Percentage'
    }
    colors = ['blue', 'green', 'red', 'purple'] # Define different colors for each subplot

    fig, axes = plt.subplots(nrows=len(percentage_columns), ncols=1, sharex=True, figsize=(12, 3 * len(percentage_columns)))

    # Set x-axis major locator to 2-minute intervals
    major_locator = mdates.MinuteLocator(interval=2)
    # Set x-axis formatter to show only HH:MM:SS
    major_formatter = mdates.DateFormatter('%H:%M:%S')


    fig.autofmt_xdate() # Auto-format date labels

    for i, metric in enumerate(percentage_columns):
        # Use fill_between to fill the area under the curve
        axes[i].fill_between(df['Timestamp'], df[metric], color=colors[i], alpha=0.3)
        # Optionally, plot the line as well for clarity
        axes[i].plot(df['Timestamp'], df[metric], color=colors[i], linewidth=1)

        axes[i].set_ylabel(y_axis_labels[metric])
        axes[i].set_title(subplot_titles[metric]) # Set individual subplot titles
        axes[i].grid(True)
        axes[i].xaxis.set_major_locator(major_locator) # Apply the locator to each subplot
        axes[i].xaxis.set_major_formatter(major_formatter) # Apply the formatter to each subplot


    axes[-1].set_xlabel("Time") # Set common x-axis label at the bottom

    # Get the date from the first timestamp
    session_date = df['Timestamp'].iloc[0].strftime('%Y-%m-%d')
    main_title = f"EEG Monitoring - Brain Wave Frequencies Over Time\nTotal session on {session_date}"
    fig.suptitle(main_title, y=1.02) # Overall title


    plt.tight_layout()
    plt.show()

# Example Usage (assuming processed_eeg_df is available)
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    plot_brain_wave_frequencies(processed_eeg_df)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

def plot_continuous_eeg_signal(df):
    """
    Plots the continuous EEG signal by concatenating EEG values from each row.

    Args:
        df (pd.DataFrame): The input DataFrame with an 'EEG_Values' column.
    """
    if 'EEG_Values' not in df.columns:
        print("Error: DataFrame must contain an 'EEG_Values' column.")
        return

    # Concatenate all EEG_Values arrays into a single array
    all_eeg_values = np.concatenate(df['EEG_Values'].values)

    plt.figure(figsize=(15, 6))
    plt.plot(all_eeg_values)
    plt.title("Continuous EEG Signal")
    plt.xlabel("Sample Index")
    plt.ylabel("EEG Value")
    plt.grid(True)
    plt.show()

# Example Usage (assuming processed_eeg_df is available)
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    plot_continuous_eeg_signal(processed_eeg_df)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np

def plot_combined_metrics_eeg(df, start_time, end_time):
    """
    Plots time series subplots for CSI, EMG, SQI, BS, and the continuous EEG signal
    for a specified time window, aligned by timestamp in a grid layout,
    with adjusted colors, subplot order, and height ratios.

    Args:
        df (pd.DataFrame): The processed DataFrame with 'Timestamp', 'CSI', 'EMG', 'SQI',
                           'BS', and 'EEG_Values' columns.
        start_time (pd.Timestamp): The start of the time window for plotting.
        end_time (pd.Timestamp): The end of the time window for plotting.
    """
    if not all(col in df.columns for col in ['Timestamp', 'CSI', 'EMG', 'SQI', 'BS', 'EEG_Values']):
        print("Error: DataFrame must contain 'Timestamp', 'CSI', 'EMG', 'SQI', 'BS', and 'EEG_Values' columns.")
        return

    # Select data for the time window
    df_window = df[(df['Timestamp'] >= start_time) & (df['Timestamp'] <= end_time)].copy()

    if df_window.empty:
        print(f"No data found for the time window: {start_time} to {end_time}")
        return

    # Prepare data for subplots
    metrics_to_plot = ['CSI', 'EMG', 'SQI', 'BS']
    eeg_values_window = np.concatenate(df_window['EEG_Values'].values)

    # Define subplot height ratios
    height_ratios = [2, 1, 1, 1, 1] # EEG, CSI, EMG, SQI, BS

    # Create a grid of subplots with specified height ratios
    fig, axes = plt.subplots(nrows=len(metrics_to_plot) + 1, ncols=1, sharex=False, figsize=(15, 12), gridspec_kw={'height_ratios': height_ratios})

    # Define a list of distinct colors for the plots
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'] # Example distinct colors


    # Plot the continuous EEG signal (top plot)
    # Create a time index for the concatenated EEG values
    # Assuming uniform sampling rate estimated from validation results
    # This is an approximation and might need adjustment based on actual sample rate
    if 'eeg_validation_results' in locals() and eeg_validation_results is not None and eeg_validation_results['estimated_sampling_rate_hz'] > 0:
        sampling_rate = eeg_validation_results['estimated_sampling_rate_hz']
        # Calculate the time for each EEG sample
        time_index_eeg = pd.to_datetime(df_window['Timestamp'].iloc[0]) + pd.to_timedelta(np.arange(len(eeg_values_window)) / sampling_rate, unit='s')

        axes[0].plot(time_index_eeg, eeg_values_window, color=colors[0])
        axes[0].set_ylabel("EEG Value")
        axes[0].set_title("Continuous EEG Signal")
        axes[0].grid(True)
        axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
        axes[0].xaxis.set_major_locator(mdates.AutoDateLocator()) # Use AutoDateLocator for flexibility


        # Plot the main metrics (CSI, EMG, SQI, BS) below the EEG plot
        for i, metric in enumerate(metrics_to_plot):
            axes[i+1].plot(df_window['Timestamp'], df_window[metric], color=colors[i+1])
            axes[i+1].set_ylabel(metric)
            axes[i+1].set_title(f'{metric} over Time')
            axes[i+1].grid(True)
            axes[i+1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
            axes[i+1].xaxis.set_major_locator(mdates.AutoDateLocator()) # Use AutoDateLocator for flexibility

            # Attempt to share the x-axis after plotting to align the time ranges
            axes[i+1].set_xlim(time_index_eeg.min(), time_index_eeg.max())

    else:
        print("Could not estimate sampling rate or processed_eeg_df not found for EEG time alignment.")
        axes[0].plot(eeg_values_window, color=colors[0])
        axes[0].set_ylabel("EEG Value")
        axes[0].set_title("Continuous EEG Signal (Time alignment not available)")
        axes[0].grid(True)
        axes[0].set_xlabel("Sample Index")

        # Plot the main metrics (CSI, EMG, SQI, BS) below the EEG plot
        for i, metric in enumerate(metrics_to_plot):
            axes[i+1].plot(df_window['Timestamp'], df_window[metric], color=colors[i+1])
            axes[i+1].set_ylabel(metric)
            axes[i+1].set_title(f'{metric} over Time')
            axes[i+1].grid(True)
            axes[i+1].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
            axes[i+1].xaxis.set_major_locator(mdates.AutoDateLocator()) # Use AutoDateLocator for flexibility


    axes[-1].set_xlabel("Time") # Set common x-axis label at the bottom
    fig.suptitle("Combined Metrics and EEG Signal Over Time Window", y=1.02)
    plt.tight_layout()
    plt.show()

# Example Usage (assuming processed_eeg_df is available)
# Define a time window for plotting
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    # Example time window (adjust as needed)
    plot_start_time = processed_eeg_df['Timestamp'].min() # Use minimum timestamp for start
    plot_end_time = processed_eeg_df['Timestamp'].max() # Use maximum timestamp for end

    plot_combined_metrics_eeg(processed_eeg_df, plot_start_time, plot_end_time)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
import seaborn as sns

def _to_rectangular(matrix_of_lists):
    lengths = [len(x) for x in matrix_of_lists]
    if len(set(lengths)) != 1:
        target = max(set(lengths), key=lengths.count)
        matrix_of_lists = [
            x[:target] if len(x) >= target else x + [np.nan] * (target - len(x))
            for x in matrix_of_lists
        ]
    arr = np.asarray(matrix_of_lists, dtype=float)
    return arr

def plot_combined_metrics_eeg_heatmap_aligned(df, start_time, end_time):
    cols = ['Timestamp', 'CSI', 'EMG', 'SQI', 'BS', 'EEG_Values']
    if not all(c in df.columns for c in cols):
        raise ValueError(f"DataFrame must contain {cols}")

    w = df[(df['Timestamp'] >= start_time) & (df['Timestamp'] <= end_time)].copy()
    if w.empty:
        print(f"No data found for {start_time} to {end_time}")
        return

    # Clean timestamps
    ts = pd.to_datetime(w['Timestamp'], errors='coerce')
    if ts.isna().any():
        raise ValueError("Some Timestamp values could not be parsed. Check your input.")
    try:
        ts = ts.dt.tz_localize(None)   # drop tz if present
    except (TypeError, AttributeError):
        pass

    # EEG matrix [samples, timestamps]
    eeg = _to_rectangular(w['EEG_Values'].tolist()).T
    if not np.issubdtype(eeg.dtype, np.number):
        eeg = eeg.astype(float)

    # Convert to Matplotlib date numbers
    t = mdates.date2num(ts.dt.to_pydatetime())
    t0, t1 = t[0], t[-1]
    # small epsilon so the rightmost column is not half-clipped
    if len(t) > 1:
        dt = (t1 - t0) / (len(t) - 1)
    else:
        dt = 1.0
    s0, s1 = 0, eeg.shape[0]

    fig = plt.figure(figsize=(15, 12))
    gs = fig.add_gridspec(5, 1, height_ratios=[2, 1, 1, 1, 1], hspace=0.35)

    ax0 = fig.add_subplot(gs[0, 0])
    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)
    ax2 = fig.add_subplot(gs[2, 0], sharex=ax0)
    ax3 = fig.add_subplot(gs[3, 0], sharex=ax0)
    ax4 = fig.add_subplot(gs[4, 0], sharex=ax0)

    im = ax0.imshow(
        eeg,
        aspect='auto',
        origin='upper',
        extent=[t0, t1 + dt, s1, s0],   # left, right(+dt), top, bottom
        cmap='seismic'
        # Removed cbar_kws and cbar_ax to hide the colorbar
    )

    ax0.set_title("EEG Signal Intensity Heatmap")
    ax0.set_ylabel("EEG Sample Index")

    # Time-series
    for ax, metric, color in zip([ax1, ax2, ax3, ax4],
                                 ['CSI', 'EMG', 'SQI', 'BS'],
                                 ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']):
        ax.plot(ts, w[metric].values, color=color, linewidth=1.2)
        ax.set_ylabel(metric)
        ax.set_title(f'{metric} over Time')
        ax.grid(True)

    ax4.set_xlabel("Time")
    for ax in [ax1, ax2, ax3, ax4]:
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())

    ax1.set_xlim(ts.iloc[0], ts.iloc[-1])
    fig.suptitle("Combined Metrics and EEG Signal Heatmap Over Time Window", y=0.995)
    plt.tight_layout() # Use tight_layout without adjusting for colorbar
    plt.show()

# Example Usage (assuming processed_eeg_df is available)
# Define a time window for plotting
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    # Example time window (adjust as needed)
    plot_start_time = processed_eeg_df['Timestamp'].min() # Use minimum timestamp for start
    plot_end_time = processed_eeg_df['Timestamp'].min() + pd.Timedelta(minutes=5) # Example: plot first 5 minutes

    plot_combined_metrics_eeg_heatmap_aligned(processed_eeg_df, plot_start_time, plot_end_time)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

# Example Usage (assuming processed_eeg_df is available)
# Define a time window for plotting
if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    # Example time window (adjust as needed)
    plot_start_time = processed_eeg_df['Timestamp'].min() # Use minimum timestamp for start
    plot_end_time = processed_eeg_df['Timestamp'].max() # Use maximum timestamp for end

    plot_combined_metrics_eeg_heatmap_aligned(processed_eeg_df,
                                              plot_start_time, plot_end_time)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pprint

def comprehensive_eeg_analysis(df, brain_wave_thresholds):
    """
    Performs a comprehensive statistical analysis on a given EEG dataframe.

    Args:
        df (pd.DataFrame): The processed EEG DataFrame.
        brain_wave_thresholds (dict): A dictionary of thresholds for brain wave analysis
                                      (e.g., {'Delta(%)': 50, 'Theta(%)': 30}).

    Returns:
        dict: A dictionary containing the results of the comprehensive analysis.
    """
    analysis_results = {}

    # 1. Calculate Basic Statistics for Metrics
    metrics_to_analyze = ['CSI', 'EMG', 'SQI', 'BS', 'Consciousness Score']
    # Ensure 'Consciousness Score' exists, calculate if not
    if 'Consciousness Score' not in df.columns:
         # Assuming default weights if not calculated elsewhere.
         # In a real scenario, weights might be passed as an argument or calculated based on context.
         # For this example, we'll use the weights from the previous plotting function if available, or default.
         if 'consciousness_weights' in globals():
              weights = globals()['consciousness_weights']
         else:
              weights = {'CSI': 0.6, 'EMG': 0.3, 'SQI': 0.1, 'BS': 0.0} # Default weights if not found

         if all(col in df.columns for col in weights.keys()):
             df['Consciousness Score'] = (
                df['CSI'] * weights.get('CSI', 0) +
                df['EMG'] * weights.get('EMG', 0) +
                df['SQI'] * weights.get('SQI', 0) +
                df['BS'] * weights.get('BS', 0)
            )
             analysis_results["Basic Statistics - Metrics"] = df[metrics_to_analyze].describe().to_dict()
         else:
             print("Warning: Could not calculate 'Consciousness Score'. Required columns for weights are missing.")
             analysis_results["Basic Statistics - Metrics"] = df[[col for col in metrics_to_analyze if col in df.columns]].describe().to_dict()

    else:
        analysis_results["Basic Statistics - Metrics"] = df[metrics_to_analyze].describe().to_dict()

    # 2. Calculate Basic Statistics for Brain Waves
    percentage_columns = ['Delta(%)', 'Theta(%)', 'Beta(%)', 'Alpha(%)']
    analysis_results["Basic Statistics - Brain Waves"] = df[percentage_columns].describe().to_dict()


    # 3. Perform Trend Analysis (using rolling mean as an example)
    trend_metrics = ['CSI', 'EMG', 'SQI', 'BS']
    window_size = 60 # Example window size (adjust based on data frequency)
    trend_analysis_results = {}
    for metric in trend_metrics:
        if metric in df.columns and 'Timestamp' in df.columns:
             # Calculate rolling mean
            rolling_mean = df[metric].rolling(window=window_size).mean()
            # Store or analyze the rolling mean (for this function, we'll just note its existence)
            # In a full analysis, you might want to store the rolling mean series or extract trend features
            trend_analysis_results[f'{metric}_rolling_mean_calculated'] = True # Placeholder indication
        else:
            trend_analysis_results[f'{metric}_trend_analysis'] = f"Column '{metric}' or 'Timestamp' not found for trend analysis."

    analysis_results["Trend Analysis"] = trend_analysis_results

    # 4. Analyze High Brain Wave Frequencies based on Thresholds
    brain_wave_threshold_analysis = {}
    for brain_wave_col, threshold in brain_wave_thresholds.items():
        if brain_wave_col in df.columns:
            # Filter the DataFrame for values above the threshold
            above_threshold_df = df[df[brain_wave_col] > threshold]

            # Count occurrences above the threshold
            occurrences_above_threshold = len(above_threshold_df)
            brain_wave_threshold_analysis[f'{brain_wave_col}_occurrences_above_{threshold}%'] = occurrences_above_threshold

            # Calculate the average percentage when above the threshold
            if occurrences_above_threshold > 0:
                average_percentage_above_threshold = above_threshold_df[brain_wave_col].mean()
                brain_wave_threshold_analysis[f'{brain_wave_col}_average_above_{threshold}%'] = average_percentage_above_threshold
            else:
                brain_wave_threshold_analysis[f'{brain_wave_col}_average_above_{threshold}%'] = None
        else:
            print(f"Warning: Column '{brain_wave_col}' not found in the DataFrame for threshold analysis.")
    analysis_results["Brain Wave Threshold Analysis"] = brain_wave_threshold_analysis

    # 5. Calculate EEG Value-Based Statistics
    eeg_value_stats = {}
    if 'EEG_Min' in df.columns and 'EEG_Max' in df.columns:
        eeg_value_stats["Average EEG Minimum Value"] = df['EEG_Min'].mean()
        eeg_value_stats["Average EEG Maximum Value"] = df['EEG_Max'].mean()
    else:
        eeg_value_stats["EEG Value-Based Statistics"] = "Required 'EEG_Min' or 'EEG_Max' columns not found."

    analysis_results["EEG Value-Based Statistics"] = eeg_value_stats


    return analysis_results

# Example Usage
# Define example thresholds
analysis_thresholds = {
    'Delta(%)': 50,
    'Theta(%)': 30,
    'Beta(%)': 20,
    'Alpha(%)': 25
}

if 'processed_eeg_df' in locals() and processed_eeg_df is not None:
    # Ensure 'Consciousness Score' is calculated in the main DataFrame before passing to the function
    # This avoids recalculating with default weights if custom weights were used earlier
    if 'Consciousness Score' not in processed_eeg_df.columns:
         if 'consciousness_weights' in globals():
              weights = globals()['consciousness_weights']
         else:
              weights = {'CSI': 0.6, 'EMG': 0.3, 'SQI': 0.1, 'BS': 0.0} # Default weights

         if all(col in processed_eeg_df.columns for col in weights.keys()):
             processed_eeg_df['Consciousness Score'] = (
                processed_eeg_df['CSI'] * weights.get('CSI', 0) +
                processed_eeg_df['EMG'] * weights.get('EMG', 0) +
                processed_eeg_df['SQI'] * weights.get('SQI', 0) +
                processed_eeg_df['BS'] * weights.get('BS', 0)
            )
         else:
            print("Warning: Could not calculate 'Consciousness Score' for the main DataFrame.")

    comprehensive_results = comprehensive_eeg_analysis(processed_eeg_df, analysis_thresholds)
    print("Comprehensive EEG Data Analysis Results:")
    pprint.pprint(comprehensive_results)
else:
    print("Processed EEG DataFrame not found. Please run the previous cells to create it.")

import json
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.dates import DateFormatter
from pathlib import Path
import warnings

def process_hrv_json(json_path, save_summary_csv=False, save_trend_plot=False, show_plot=False):
    """
    Processes a specific VibeScience HRV JSON file to extract metrics,
    calculate summaries, and optionally save/show a trend plot.

    Args:
        json_path (str or Path): Path to the input HRV JSON file.
        save_summary_csv (bool): If True, saves summary CSV files.
        save_trend_plot (bool): If True, saves a trend plot PNG.
        show_plot (bool): If True, displays the trend plot interactively
                          after saving (uses plt.show()).

    Returns:
        tuple: A tuple containing (summary_phase, summary_overall, delta)
               DataFrames. Returns (None, None, None) if no data is found.
    """

    # --- 1. Path and File Setup ---
    json_path = Path(json_path)
    if not json_path.exists():
        print(f"Error: File not found at {json_path}")
        return None, None, None

    # Define output paths
    OUT_PNG = json_path.with_name(f"{json_path.stem}_hrv_trends.png")
    OUT_CSV_PHASE = json_path.with_name(f"{json_path.stem}_hrv_summary_phase.csv")
    OUT_CSV_OVERALL = json_path.with_name(f"{json_path.stem}_hrv_summary_overall.csv")
    OUT_CSV_DELTA = json_path.with_name(f"{json_path.stem}_hrv_summary_delta.csv")

    # --- 2. Load and Flatten Data ---
    try:
        with open(json_path, "r") as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error reading or parsing JSON file: {e}")
        return None, None, None

    records = []
    for detail in data.get("details", []):
        phase = detail.get("testType", "unknown")
        for metric in detail.get("metrics", []):
            raw = metric.get("raw", {})
            if not raw:
                continue
            row = {**raw}
            row["testType"] = phase
            row["timestamp"] = pd.to_datetime(metric.get("on"))
            records.append(row)

    if not records:
        print("No HRV metric records found in the file.")
        return None, None, None

    df = pd.DataFrame(records).sort_values("timestamp").reset_index(drop=True)

    # --- NEW: Get date for plot title ---
    plot_date = df["timestamp"].min().strftime("%Y-%m-%d")

    # --- 3. Generate Summaries ---
    want_cols = [
        "meanHR", "sdnn", "rmssd", "pnn20", "pnn50", "lfPower", "hfPower",
        "lfHfRatio", "stressScore", "recoveryScore", "triangularIndex",
    ]
    num_cols = [c for c in want_cols if c in df.columns]

    summary_phase = (
        df.groupby("testType")[num_cols]
          .mean()
          .round(2)
    )
    phase_order = sorted(summary_phase.index, key=lambda x: ("pre" not in x, "live" not in x, "post" not in x, x))
    summary_phase = summary_phase.reindex(phase_order)

    summary_overall = df[num_cols].mean().to_frame("overall_mean").round(2)

    # (Improved) deltas
    pre_rows = df[df['testType'].str.contains('preRun', case=False, na=False)]
    post_rows = df[df['testType'].str.contains('postRun', case=False, na=False)]

    delta = None
    if not pre_rows.empty and not post_rows.empty:
        pre_mean = pre_rows[num_cols].mean()
        post_mean = post_rows[num_cols].mean()
        delta = (post_mean - pre_mean).to_frame("postRun_minus_preRun_MEAN").round(2)

    # --- 4. (Optional) Save Summaries ---
    if save_summary_csv:
        try:
            summary_phase.to_csv(OUT_CSV_PHASE, index=True)
            print(f"Saved phase summary → {OUT_CSV_PHASE}")
            summary_overall.to_csv(OUT_CSV_OVERALL, index=True)
            print(f"Saved overall summary → {OUT_CSV_OVERALL}")
            if delta is not None:
                delta.to_csv(OUT_CSV_DELTA, index=True)
                print(f"Saved delta summary → {OUT_CSV_DELTA}")
        except Exception as e:
            print(f"Error saving summary CSVs: {e}")

    # --- 5. (Optional) Plot Trends (Subplots) ---
    if save_trend_plot or show_plot:
        plot_cols = [c for c in ["meanHR", "sdnn", "rmssd", "lfHfRatio", "stressScore", "recoveryScore"] if c in df.columns]

        if not plot_cols:
            print("No data columns found for plotting.")
        else:
            num_plots = len(plot_cols)
            fig, axes = plt.subplots(
                num_plots, 1,
                figsize=(14, num_plots * 2.5),
                sharex=True,
                squeeze=False
            )
            axes = axes.flatten()

            # --- NEW: Define grouped colors for boundaries ---
            phases = sorted(df['testType'].unique())
            color_variants = {
                'pre': ['#a2d5ab', '#6aa84f'], # Shades of green
                'live': ['#a9d0f5', '#6fa8dc'], # Shades of blue
                'post': ['#f4cccc', '#e06666']  # Shades of red/coral
            }
            phase_colors = {}
            counts = {'pre': 0, 'live': 0, 'post': 0}

            for phase in phases:
                phase_type = 'live' # default
                if 'pre' in phase.lower():
                    phase_type = 'pre'
                elif 'post' in phase.lower():
                    phase_type = 'post'

                color_list = color_variants[phase_type]
                color_index = counts[phase_type] % len(color_list)
                phase_colors[phase] = color_list[color_index]
                counts[phase_type] += 1

            # Plot each metric on its own subplot
            for i, col in enumerate(plot_cols):
                ax = axes[i]
                plot_color = f'C{i}' # Use a different base color for each plot

                ax.plot(df["timestamp"], df[col], label=col, color=plot_color)
                ax.set_ylabel(col)
                ax.legend(loc='upper left')
                ax.grid(True, alpha=0.4, linestyle='--')

                # Add shaded regions (color boxes) for each phase
                for phase, g in df.groupby("testType"):
                    start, end = g["timestamp"].min(), g["timestamp"].max()
                    ax.axvspan(
                        start, end,
                        color=phase_colors.get(phase, 'gray'),
                        alpha=0.4, # Made slightly more opaque
                        zorder=-1
                    )

            # --- NEW: Set X-axis format and plot title ---
            axes[-1].xaxis.set_major_formatter(DateFormatter('%H:%M:%S'))
            axes[-1].set_xlabel("Timestamp (HH:MM:SS)")

            # Create a single figure legend for the phase windows
            legend_patches = [
                mpatches.Patch(color=c, alpha=0.4, label=f"{p} window")
                for p, c in phase_colors.items()
            ]
            fig.legend(
                handles=legend_patches,
                loc='upper center',
                bbox_to_anchor=(0.5, 0.98),
                ncol=len(phases),
                fontsize=10
            )

            # --- NEW: Set dynamic title ---
            fig.suptitle(f"HRV Trends on {plot_date}", y=1.03, fontsize=16)

            try:
                plt.tight_layout(rect=[0, 0, 1, 0.95])

                if save_trend_plot:
                    plt.savefig(OUT_PNG, dpi=150, bbox_inches='tight')
                    print(f"Saved plot → {OUT_PNG}")

                if show_plot:
                    print("Displaying plot...")
                    plt.show()

            except Exception as e:
                print(f"Error saving/showing plot: {e}")
            finally:
                if not show_plot:
                    plt.close(fig)

    # --- 6. Return Data ---
    return df, summary_phase, summary_overall, delta


# --- Example Usage ---
# This assumes the JSON file is in the same directory as the script
# Update this path if your file is located elsewhere
FILE = hrv_file[0]

# Suppress Matplotlib UserWarning for colormap
warnings.filterwarnings("ignore", message="The get_cmap function was deprecated")

if Path(FILE).exists():
    # Run the function
    df_hrv, summary_phase, summary_overall, delta = process_hrv_json(
        json_path=FILE,
        save_summary_csv=True,
        save_trend_plot=True,
        show_plot=True
    )

    # You can now work with the returned DataFrames
    if summary_phase is not None:
        print("\n--- Summary by Phase ---")
        print(summary_phase)

    if summary_overall is not None:
        print("\n--- Overall Mean ---")
        print(summary_overall)

    if delta is not None:
        print("\n--- Delta (Post-Run vs Pre-Run) ---")
        print(delta)
else:
    print(f"Example file '{FILE}' not found.")
    print("Please download the file or update the 'FILE' variable.")

# eeg_file, hrv_file

# df_hrv.head()

# df_hrv.columns

first_timestamp = df_hrv['timestamp'].min()
last_timestamp = df_hrv['timestamp'].max()

print(f"First Timestamp: {first_timestamp}")
print(f"Last Timestamp: {last_timestamp}")

first_timestamp = processed_eeg_df['Timestamp'].min()
last_timestamp = processed_eeg_df['Timestamp'].max()

print(f"First Timestamp: {first_timestamp}")
print(f"Last Timestamp: {last_timestamp}")

"""Check HRV data as DF"""

import json
import pandas as pd
from pathlib import Path

def load_hrv_json_to_df(json_path):
    """
    Reads a VibeScience HRV JSON file and flattens it into a pandas DataFrame.

    The function extracts all raw metrics from each phase ("details" block)
    and organizes them into a single DataFrame, sorted by time.

    Args:
        json_path (str or Path): The file path to the HRV JSON file.

    Returns:
        pd.DataFrame: A DataFrame where each row is a time-stamped HRV metric.
                      - 'timestamp' is the first column.
                      - 'testType' (phase) is the second column.
                      - Returns an empty DataFrame if the file is not found,
                        is invalid, or contains no HRV metrics.
    """
    json_path = Path(json_path)
    if not json_path.exists():
        print(f"Error: File not found at {json_path}")
        return pd.DataFrame()  # Return an empty DataFrame

    try:
        with open(json_path, "r") as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error reading or parsing JSON file: {e}")
        return pd.DataFrame()  # Return an empty DataFrame

    records = []
    # Iterate through each phase (preRun1, liveRun, etc.)
    for detail in data.get("details", []):
        phase = detail.get("testType", "unknown")

        # Iterate through each metric snapshot in that phase
        for metric in detail.get("metrics", []):
            raw = metric.get("raw", {})
            if not raw:
                continue

            # Start with all the raw HRV data (sdnn, rmssd, etc.)
            row = {**raw}

            # Add the high-level context
            row["testType"] = phase
            row["timestamp"] = pd.to_datetime(metric.get("on"))
            records.append(row)

    if not records:
        print("No HRV metric records found in the file.")
        return pd.DataFrame()

    # Create the DataFrame
    df = pd.DataFrame(records)

    # Sort by timestamp to ensure chronological order
    df = df.sort_values("timestamp").reset_index(drop=True)

    # --- Reorder columns to meet the requirement ---

    # Get a list of all columns
    all_columns = list(df.columns)

    # Define the columns you want to see first
    # This automatically handles the "timestamp as first column" rule
    first_cols = ['timestamp', 'testType']

    # Get all other columns that weren't in the 'first_cols' list
    other_cols = [col for col in all_columns if col not in first_cols]

    # Create the new column order and apply it
    new_col_order = first_cols + other_cols
    df = df[new_col_order]

    return df


if Path(FILE).exists():
    # Call the new function
    hrv_df = load_hrv_json_to_df(FILE)

    if not hrv_df.empty:
        print(f"Successfully loaded DataFrame with {len(hrv_df)} records.")

        # Display the first 5 rows and the column names
        print("\n--- DataFrame Head ---")
        print(hrv_df.head())

        print("\n--- DataFrame Columns ---")
        print(list(hrv_df.columns))
    else:
        print("Failed to load or DataFrame is empty.")
else:
    print(f"Example file '{FILE}' not found.")

"""Get timing details of HRV data."""

# --- NEW Function 2: As requested ---

# --- Function 2: As requested (Now with 'duration') ---

def get_testType_timings(hrv_df):
    """
    Calculates the start time, end time, and total duration for each
    unique testType in the provided HRV DataFrame.

    Args:
        hrv_df (pd.DataFrame): The DataFrame loaded by load_hrv_json_to_df,
                               must contain 'timestamp' and 'testType' columns.

    Returns:
        pd.DataFrame: A DataFrame indexed by 'testType' with 'start_time',
                      'end_time', and 'duration' columns.
                      Returns an empty DataFrame on error.
    """
    # --- Input Validation ---
    if hrv_df.empty:
        print("Error: Input DataFrame is empty.")
        return pd.DataFrame()

    if 'timestamp' not in hrv_df.columns or 'testType' not in hrv_df.columns:
        print("Error: DataFrame must contain 'timestamp' and 'testType' columns.")
        return pd.DataFrame()

    # --- Core Logic ---
    try:
        # Group by 'testType' and get the min/max timestamps
        timings = hrv_df.groupby('testType')['timestamp'].agg(
            start_time='min',
            end_time='max'
        )

        # --- NEW: Calculate the duration ---
        # This creates a new 'duration' column
        timings['duration'] = timings['end_time'] - timings['start_time']

        # Sort the results by the start time to see the chronological order
        timings = timings.sort_values(by='start_time')

        return timings

    except Exception as e:
        print(f"An error occurred during aggregation: {e}")
        return pd.DataFrame()


# --- Example Usage ---

if Path(FILE).exists():
    # 1. Load the main DataFrame using the first function
    print(f"Loading data from {FILE}...")
    hrv_df = load_hrv_json_to_df(FILE)

    if not hrv_df.empty:
        print("Data loaded successfully.")

        # 2. Get the timings using the new (second) function
        print("\nCalculating testType timings...")
        timings_df = get_testType_timings(hrv_df)

        if not timings_df.empty:
            print("\n--- Start and End Times by testType ---")
            print(timings_df)
    else:
        print("Failed to load or DataFrame is empty.")
else:
    print(f"Example file '{FILE}' not found.")

"""Validate timestamps of each dataframe"""

# --- NEW Function 3: As requested ---

def validate_timestamp_alignment(eeg_df, hrv_df):
    """
    Validates that the EEG DataFrame's timestamps fully envelop the
    HRV DataFrame's timestamps.

    It checks for two conditions:
    1. EEG start time <= HRV start time
    2. EEG finish time >= HRV finish time

    Args:
        eeg_df (pd.DataFrame): The loaded EEG DataFrame. Must have a 'timestamp' column.
        hrv_df (pd.DataFrame): The loaded HRV DataFrame. Must have a 'timestamp' column.

    Returns:
        bool: True if both conditions are met, False otherwise.
    """

    # --- 1. Input Validation ---
    if eeg_df.empty:
        print("Validation FAILED: EEG DataFrame is empty.")
        return False
    if hrv_df.empty:
        print("Validation FAILED: HRV DataFrame is empty.")
        return False

    if 'Timestamp' not in eeg_df.columns:
        print("Validation FAILED: EEG DataFrame is missing 'timestamp' column.")
        return False
    if 'timestamp' not in hrv_df.columns:
        print("Validation FAILED: HRV DataFrame is missing 'timestamp' column.")
        return False

    # --- 2. Get Min and Max Timestamps ---
    try:
        hrv_start = hrv_df['timestamp'].min()
        hrv_finish = hrv_df['timestamp'].max()
        eeg_start = eeg_df['Timestamp'].min()
        eeg_finish = eeg_df['Timestamp'].max()
    except Exception as e:
        print(f"Validation FAILED: Could not get min/max timestamps. Error: {e}")
        return False

    # --- 3. Perform Validation and Print Report ---
    print("--- Timestamp Alignment Validation ---")
    print(f"  HRV Start:  {hrv_start}")
    print(f"  EEG Start:  {eeg_start}")

    is_start_aligned = eeg_start <= hrv_start
    if is_start_aligned:
        print("  ✅ START time is aligned (EEG starts on or before HRV).")
    else:
        print(f"  ❌ FAILED: EEG data starts TOO LATE (after HRV).")
        print(f"     Delta: +{eeg_start - hrv_start}")

    print("\n" + "-"*38)
    print(f"  HRV Finish: {hrv_finish}")
    print(f"  EEG Finish: {eeg_finish}")

    is_finish_aligned = eeg_finish >= hrv_finish
    if is_finish_aligned:
        print("  ✅ FINISH time is aligned (EEG finishes on or after HRV).")
    else:
        print(f"  ❌ FAILED: EEG data finishes TOO EARLY (before HRV).")
        print(f"     Delta: -{hrv_finish - eeg_finish}")

    print("--- End of Validation Report ---")

    # --- 4. Return Final Result ---
    return is_start_aligned and is_finish_aligned


# --- Example Usage ---


if hrv_df.empty:
    print("Failed to load HRV data. Exiting.")
else:
    print("HRV data loaded successfully.")
    hrv_start = hrv_df['timestamp'].min()
    hrv_finish = hrv_df['timestamp'].max()

    # 2. --- SIMULATE EEG DATAFRAMES FOR TESTING ---

    # Scenario 1: GOOD (EEG envelops HRV)
    eeg_good_start = hrv_start - pd.Timedelta(minutes=1)
    eeg_good_finish = hrv_finish + pd.Timedelta(minutes=1)
    eeg_df_good = pd.DataFrame({
        'Timestamp': [eeg_good_start, eeg_good_finish],
        'EEG_Signal': [0, 0]
    })

    # Scenario 2: BAD START (EEG starts late)
    eeg_bad_start = hrv_start + pd.Timedelta(seconds=30)
    eeg_df_bad_start = pd.DataFrame({
        'Timestamp': [eeg_bad_start, eeg_good_finish],
        'EEG_Signal': [0, 0]
    })

    # Scenario 3: BAD FINISH (EEG finishes early)
    eeg_bad_finish_time = hrv_finish - pd.Timedelta(seconds=30)
    eeg_df_bad_finish = pd.DataFrame({
        'Timestamp': [eeg_good_start, eeg_bad_finish_time],
        'EEG_Signal': [0, 0]
    })

    # 3. --- Run Validation ---

    print("\n\n--- Running Validation: SCENARIO 1 (GOOD) ---")
    is_valid_1 = validate_timestamp_alignment(eeg_df_good, hrv_df)
    print(f"Final Result (Good): {is_valid_1}")

    print("\n\n--- Running Validation: SCENARIO 2 (BAD START) ---")
    is_valid_2 = validate_timestamp_alignment(eeg_df_bad_start, hrv_df)
    print(f"Final Result (Bad Start): {is_valid_2}")

    print("\n\n--- Running Validation: SCENARIO 3 (BAD FINISH) ---")
    is_valid_3 = validate_timestamp_alignment(eeg_df_bad_finish, hrv_df)
    print(f"Final Result (Bad Finish): {is_valid_3}")

print("\n\n--- Running Validation: ACTUAL ---")
is_valid = validate_timestamp_alignment(processed_eeg_df, hrv_df)
print(f"Final Result: {is_valid}")

"""Assign test types"""

import numpy as np # Used for the example

# --- NEW Function 4: As requested ---

def assign_hrv_testTypes_to_eeg(eeg_df, hrv_timings_df):
    """
    Assigns testType labels from HRV timings to an EEG DataFrame.

    Creates a new 'testType' column in the EEG DataFrame.
    - Rows with timestamps *within* a phase's start/end time are labeled
      with that phase's testType.
    - All other rows are labeled 'OUT'.

    Args:
        eeg_df (pd.DataFrame): The EEG DataFrame. Must have a 'timestamp' column.
        hrv_timings_df (pd.DataFrame): The timings DataFrame from
                                     get_testType_timings(). Must have
                                     'start_time', 'end_time' as columns
                                     and testType as the index.

    Returns:
        pd.DataFrame: A *new* copy of the EEG DataFrame with the 'testType'
                      column added and populated.
    """

    # --- 1. Validation ---
    if eeg_df.empty:
        print("Error: EEG DataFrame is empty.")
        return pd.DataFrame()
    if 'Timestamp' not in eeg_df.columns:
        print("Error: EEG DataFrame is missing 'Timestamp' column.")
        return pd.DataFrame()

    if hrv_timings_df.empty:
        print("Error: HRV Timings DataFrame is empty.")
        return pd.DataFrame()
    if not {'start_time', 'end_time'}.issubset(hrv_timings_df.columns):
        print("Error: HRV Timings DataFrame must have 'start_time' and 'end_time' columns.")
        return pd.DataFrame()

    # --- 2. Create the labeled DataFrame ---

    # Make a copy to avoid changing the original DataFrame
    eeg_df_labeled = eeg_df.copy()

    # Initialize the new column with the default value 'OUT'
    eeg_df_labeled['testType'] = 'OUT'

    # --- 3. Iterate and Label ---

    print("Labeling EEG data based on HRV phases...")
    # Iterate through the timings (index is the testType)
    for testType, row in hrv_timings_df.iterrows():
        start = row['start_time']
        end = row['end_time']

        # Create a boolean mask for rows within the current interval
        mask = (eeg_df_labeled['Timestamp'] >= start) & \
               (eeg_df_labeled['Timestamp'] <= end)

        # Use .loc to update the 'testType' for these rows
        eeg_df_labeled.loc[mask, 'testType'] = testType

        print(f"  - Found {mask.sum()} EEG samples for phase: {testType}")

    print("Labeling complete.")
    return eeg_df_labeled


# --- Example Usage ---

if not hrv_df.empty and not timings_df.empty:

    # --- 3. SIMULATE an EEG DataFrame ---
    print("\nSimulating EEG DataFrame...")

    # Get the *total* time range from HRV data
    session_start = timings_df['start_time'].min() - pd.Timedelta(seconds=15)
    session_end = timings_df['end_time'].max() + pd.Timedelta(seconds=15)

    # Create a high-frequency timestamp range (e.g., 250Hz -> 4ms)
    eeg_timestamps = pd.date_range(
        start=session_start,
        end=session_end,
        freq='4ms' # Simulating 250Hz sampling rate
    )

    # Create the dummy EEG DataFrame
    simulated_eeg_df = pd.DataFrame({
        'Timestamp': eeg_timestamps,
        'EEG_Signal_Fp1': np.random.rand(len(eeg_timestamps)),
        'EEG_Signal_Fp2': np.random.rand(len(eeg_timestamps))
    })
    print(f"Simulated EEG data with {len(simulated_eeg_df)} samples.")

    # --- 4. Run the new labeling function ---
    eeg_labeled_df = assign_hrv_testTypes_to_eeg(simulated_eeg_df, timings_df)

    # --- 5. Check the results ---
    if not eeg_labeled_df.empty:
        print("\n--- Result: EEG DataFrame Label Counts ---")
        # This shows how many samples were assigned to each category
        print(eeg_labeled_df['testType'].value_counts())

        print("\n--- Example: First 5 'OUT' samples (start) ---")
        print(eeg_labeled_df[eeg_labeled_df['testType'] == 'OUT'].head(5))

        print("\n--- Example: First 5 'preRun1' samples ---")
        print(eeg_labeled_df[eeg_labeled_df['testType'] == 'preRun1'].head(5))

else:
    print("Failed to load HRV data or timings. Exiting example.")

# --- 4. Run the new labeling function ---
eeg_labeled_df = assign_hrv_testTypes_to_eeg(processed_eeg_df, timings_df)

# --- 5. Check the results ---
if not eeg_labeled_df.empty:
    print("\n--- Result: EEG DataFrame Label Counts ---")
    # This shows how many samples were assigned to each category
    print(eeg_labeled_df['testType'].value_counts())

    print("\n--- Example: First 5 'OUT' samples (start) ---")
    print(eeg_labeled_df[eeg_labeled_df['testType'] == 'OUT'].head(5))

    print("\n--- Example: First 5 'preRun1' samples ---")
    print(eeg_labeled_df[eeg_labeled_df['testType'] == 'preRun1'].head(5))

"""Visualise test Types for brain waves."""

import json
import pandas as pd
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.dates import DateFormatter
import warnings

# --- NEW Function 4: As requested ---

def visualize_eeg_trends(eeg_labeled_df, columns_to_visualize,
                         save_plot=False, show_plot=False,
                         out_png_path="eeg_trends.png"):
    """
    Generates and optionally saves/shows a subplot-based trend plot for
    specified columns in the labeled EEG DataFrame.

    Args:
        eeg_labeled_df (pd.DataFrame): The EEG DataFrame with 'timestamp' and
                                     'testType' columns (from assign_hrv_testTypes_to_eeg).
        columns_to_visualize (list): A list of column names to plot
                                     (e.g., ['alpha_%', 'beta_%']).
        save_plot (bool): If True, saves the plot to out_png_path.
        show_plot (bool): If True, displays the plot interactively.
        out_png_path (str): The file path to save the plot.
    """

    # --- 1. Validation ---
    if eeg_labeled_df.empty:
        print("Plotting FAILED: EEG DataFrame is empty.")
        return

    if 'Timestamp' not in eeg_labeled_df.columns or 'testType' not in eeg_labeled_df.columns:
        print("Plotting FAILED: EEG DataFrame must have 'Timestamp' and 'testType' columns.")
        return

    # Find which of the requested columns are actually in the DataFrame
    plot_cols = [col for col in columns_to_visualize if col in eeg_labeled_df.columns]
    if not plot_cols:
        print(f"Plotting FAILED: None of the specified columns {columns_to_visualize} were found in the DataFrame.")
        return

    # --- 2. Setup Plot ---
    num_plots = len(plot_cols)
    fig, axes = plt.subplots(
        num_plots, 1,
        figsize=(14, num_plots * 2.5),
        sharex=True,
        squeeze=False
    )
    axes = axes.flatten()

    # Get date for title
    plot_date = eeg_labeled_df["Timestamp"].min().strftime("%Y-%m-%d")

    # --- 3. Define Phase Colors (Boundaries) ---
    phases = sorted(eeg_labeled_df['testType'].unique())
    color_variants = {
        'pre': ['#a2d5ab', '#6aa84f'], # Shades of green
        'live': ['#a9d0f5', '#6fa8dc'], # Shades of blue
        'post': ['#f4cccc', '#e06666'], # Shades of red/coral
        'out': ['#eeeeee']              # Neutral light gray for OUT
    }
    phase_colors = {}
    counts = {'pre': 0, 'live': 0, 'post': 0, 'out': 0}

    for phase in phases:
        phase_type = 'out' # default
        if 'pre' in phase.lower(): phase_type = 'pre'
        elif 'live' in phase.lower(): phase_type = 'live'
        elif 'post' in phase.lower(): phase_type = 'post'

        color_list = color_variants[phase_type]
        color_index = counts[phase_type] % len(color_list)
        phase_colors[phase] = color_list[color_index]
        counts[phase_type] += 1

    # --- 4. Find Consecutive Blocks for Shading ---
    # This is crucial for handling 'OUT' blocks between phases.
    # We find where the testType changes, and give each block a unique ID.
    eeg_labeled_df['block'] = (eeg_labeled_df['testType'] != eeg_labeled_df['testType'].shift()).cumsum()

    # Get the start, end, and name for each continuous block
    blocks = []
    for block_id, group in eeg_labeled_df.groupby('block'):
        blocks.append({
            'start': group['Timestamp'].min(),
            'end': group['Timestamp'].max(),
            'testType': group['testType'].iloc[0] # The name of this block
        })

    # --- 5. Create Subplots ---
    print(f"Generating plot for {', '.join(plot_cols)}...")
    for i, col in enumerate(plot_cols):
        ax = axes[i]
        plot_color = f'C{i}' # Vivid color for each line plot

        # Plot the EEG data
        ax.plot(
            eeg_labeled_df["Timestamp"],
            eeg_labeled_df[col],
            label=col,
            color=plot_color,
            linewidth=1.0 # Use a slightly thinner line for dense data
        )
        ax.set_ylabel(col)
        ax.legend(loc='upper left')
        ax.grid(True, alpha=0.4, linestyle='--')

        # Add the colored backgrounds for all blocks
        for block in blocks:
            # Use the color we defined, default to dark gray if a phase is new
            phase_color = phase_colors.get(block['testType'], '#dddddd')
            ax.axvspan(
                block['start'],
                block['end'],
                color=phase_color,
                alpha=0.4,
                zorder=-1
            )

    # --- 6. Finalize, Save, and Show ---

    # Format X-axis
    axes[-1].xaxis.set_major_formatter(DateFormatter('%H:%M:%S'))
    axes[-1].set_xlabel("Timestamp (HH:MM:SS)")

    # Create a single figure legend for the phase windows
    legend_patches = [
        mpatches.Patch(color=c, alpha=0.4, label=f"{p} window")
        for p, c in phase_colors.items()
    ]
    fig.legend(
        handles=legend_patches,
        loc='upper center',
        bbox_to_anchor=(0.5, 0.98),
        ncol=len(phases),
        fontsize=10
    )

    # Set dynamic title
    fig.suptitle(f"EEG Trends on {plot_date} with Test Types",
                 y=1.03, fontsize=16)

    try:
        plt.tight_layout(rect=[0, 0, 1, 0.95])

        if save_plot:
            plt.savefig(out_png_path, dpi=150, bbox_inches='tight')
            print(f"Saved plot → {out_png_path}")

        if show_plot:
            print("Displaying plot...")
            plt.show()

    except Exception as e:
        print(f"Error saving/showing plot: {e}")
    finally:
        if not show_plot:
            plt.close(fig) # Free memory


# --- Example Usage ---

# Suppress warnings
warnings.filterwarnings("ignore", message="The get_cmap function was deprecated")

# --- 5. Run the NEW visualization function ---
if not eeg_labeled_df.empty:

    # Define the columns we want to see
    cols_to_plot = ['Alpha(%)', 'Beta(%)', 'Theta(%)', 'Delta(%)']

    visualize_eeg_trends(
        eeg_labeled_df,
        columns_to_visualize=cols_to_plot,
        save_plot=False,
        show_plot=True, # Set to True to see the plot
        out_png_path="eeg_band_trends.png"
    )

# eeg_labeled_df.columns

"""Get more statistics of each brain wave along different test types."""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.dates import DateFormatter
import numpy as np
import warnings

def calculate_eeg_statistics(eeg_labeled_df, columns_to_visualize):
    """
    Calculates mean, min, max, and std for specified columns, grouped by
    the 'testType' label.

    Excludes the 'OUT' category from statistics.
    Sorts the testType index chronologically.

    Args:
        eeg_labeled_df (pd.DataFrame): Labeled EEG data with 'testType'
                                     and 'Timestamp' columns.
        columns_to_visualize (list): List of columns to get stats for.

    Returns:
        pd.DataFrame: A multi-index DataFrame with stats for each testType.
    """
    # --- 1. Validation ---
    if eeg_labeled_df.empty:
        print("Stats FAILED: EEG DataFrame is empty.")
        return pd.DataFrame()

    if 'testType' not in eeg_labeled_df.columns or 'Timestamp' not in eeg_labeled_df.columns:
        print("Stats FAILED: 'testType' and 'Timestamp' columns are required.")
        return pd.DataFrame()

    # --- 2. Calculate Statistics ---

    # Exclude 'OUT' rows from statistical analysis
    stats_df = eeg_labeled_df[eeg_labeled_df['testType'] != 'OUT'].copy()

    if stats_df.empty:
        print("Stats FAILED: No data found after excluding 'OUT' rows.")
        return pd.DataFrame()

    # --- NEW: Robust column conversion and validation ---
    valid_cols = []
    print("--- Checking statistics columns ---")
    for col in columns_to_visualize:
        if col not in stats_df.columns:
            print(f"  - WARNING: Column '{col}' not found. Skipping.")
            continue

        # Check if already numeric
        if pd.api.types.is_numeric_dtype(stats_df[col]):
            print(f"  - OK: Column '{col}' is numeric.")
            valid_cols.append(col)
        else:
            # If not numeric, ATTN: convert to numeric
            print(f"  - INFO: Column '{col}' is not numeric (type: {stats_df[col].dtype}). Attempting conversion...")
            # errors='coerce' will turn un-convertible strings (like 'N/A') into NaN
            stats_df[col] = pd.to_numeric(stats_df[col], errors='coerce')

            # Check if conversion worked (it might be all NaN)
            if stats_df[col].isnull().all():
                print(f"  - WARNING: Column '{col}' failed conversion (all values are NaN). Skipping.")
            else:
                print(f"  - OK: Column '{col}' successfully converted to numeric.")
                valid_cols.append(col)

    if not valid_cols:
        print("Stats FAILED: No valid numeric columns found to aggregate.")
        return pd.DataFrame()

    # --- 3. Aggregate (FIXED SYNTAX) ---
    # The original .agg(Mean='mean', ...) was incorrect for a DataFrame.
    # The correct syntax is to pass a list of functions.
    try:
        stats = stats_df.groupby('testType')[valid_cols].agg(['mean', 'min', 'max', 'std'])
    except Exception as e:
        print(f"Stats FAILED: Aggregation failed. Error: {e}")
        return pd.DataFrame()

    # 'stats' now has a MultiIndex column: ('Alpha(%)', 'mean'), ('Alpha(%)', 'min'), ...

    # --- 4. Sort and Finalize ---
    chronological_order = stats_df.groupby('testType')['Timestamp'].min().sort_values().index

    try:
        # Reorder the index (testType) chronologically
        stats = stats.reindex(chronological_order)
    except Exception as e:
        print(f"Warning: Could not chronologically sort stats. Error: {e}")

    # We return the stats DataFrame with the MultiIndex columns
    return stats.round(2)

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.dates import DateFormatter
from matplotlib.patches import Rectangle

# --- RENAMED and UPDATED ---
def visualize_eeg_trends_advanced(eeg_labeled_df, columns_to_visualize,
                                  save_plot=False, show_plot=False,
                                  out_png_path="eeg_trends.png",
                                  show_statistics=True):
    """
    Generates and optionally saves/shows a subplot-based trend plot for
    specified columns in the labeled EEG DataFrame.

    If show_statistics=True, it will overlay mean, min/max, and std dev
    on top of the time-series for each testType block.

    Expects 'Timestamp' (uppercase) and 'testType' columns.

    Args:
        eeg_labeled_df (pd.DataFrame): The EEG DataFrame with 'Timestamp' and
                                     'testType' columns.
        columns_to_visualize (list): A list of column names to plot.
        save_plot (bool): If True, saves the plot to out_png_path.
        show_plot (bool): If True, displays the plot interactively.
        out_png_path (str): The file path to save the plot.
        show_statistics (bool): If True (default), overlays the statistics.
    """

    # --- 1. Validation (Checks for 'Timestamp') ---
    if eeg_labeled_df.empty:
        print("Plotting FAILED: EEG DataFrame is empty.")
        return

    if 'Timestamp' not in eeg_labeled_df.columns or 'testType' not in eeg_labeled_df.columns:
        print("Plotting FAILED: EEG DataFrame must have 'Timestamp' and 'testType' columns.")
        return

    plot_cols = [col for col in columns_to_visualize if col in eeg_labeled_df.columns]
    if not plot_cols:
        print(f"Plotting FAILED: None of the specified columns {columns_to_visualize} were found in the DataFrame.")
        return

    # --- UPDATED: Gets date from 'Timestamp' ---
    plot_date = eeg_labeled_df["Timestamp"].min().strftime("%Y-%m-%d")

    # --- 2. Calculate Statistics (if requested) ---
    stats_df = None
    if show_statistics:
        stats_df = calculate_eeg_statistics(eeg_labeled_df, plot_cols)
        if stats_df is None or stats_df.empty:
            print("Warning: Statistics calculation failed. Plotting time-series only.")
            show_statistics = False

    # --- 3. Define Phase Colors ---
    phases = sorted(eeg_labeled_df['testType'].unique())
    color_variants = {
        'pre': ['#a2d5ab', '#6aa84f'],
        'live': ['#a9d0f5', '#6fa8dc'],
        'post': ['#f4cccc', '#e06666'],
        'out': ['#eeeeee']
    }
    phase_colors = {}
    counts = {'pre': 0, 'live': 0, 'post': 0, 'out': 0}

    for phase in phases:
        phase_type = 'out'
        if 'pre' in phase.lower(): phase_type = 'pre'
        elif 'live' in phase.lower(): phase_type = 'live'
        elif 'post' in phase.lower(): phase_type = 'post'
        color_list = color_variants[phase_type]
        color_index = counts[phase_type] % len(color_list)
        phase_colors[phase] = color_list[color_index]
        counts[phase_type] += 1

    # --- 4. Find Consecutive Blocks for Shading ---
    df_plot = eeg_labeled_df.copy()
    df_plot['block'] = (df_plot['testType'] != df_plot['testType'].shift()).cumsum()

    blocks = []
    for block_id, group in df_plot.groupby('block'):
        blocks.append({
            # --- UPDATED: Uses 'Timestamp' ---
            'start': group['Timestamp'].min(),
            'end': group['Timestamp'].max(),
            'testType': group['testType'].iloc[0]
        })

    # --- 5. Create Subplots ---
    fig, axes = plt.subplots(
        len(plot_cols), 1,
        figsize=(14, len(plot_cols) * 3),
        sharex=True,
        squeeze=False
    )
    axes = axes.flatten()

    print(f"Generating plot for {', '.join(plot_cols)}...")
    for i, col in enumerate(plot_cols):
        ax = axes[i]
        plot_color = f'C{i}'

        # --- A. Plot the main time-series line (Uses 'Timestamp') ---
        ax.plot(
            df_plot["Timestamp"],
            df_plot[col],
            label=col,
            color=plot_color,
            linewidth=1.0,
            zorder=10
        )

        stats_labels_added = {'mean': False, 'min_max': False, 'std': False}

        # --- C. Loop through blocks to add backgrounds and stats ---
        for block in blocks:
            testType = block['testType']
            start = block['start']
            end = block['end']

            phase_color = phase_colors.get(testType, '#dddddd')
            ax.axvspan(start, end, color=phase_color, alpha=0.4, zorder=-1)

            if show_statistics and testType != 'OUT':
                try:
                    phase_stats = stats_df[col].loc[testType]
                    mean_val = phase_stats['mean']
                    min_val = phase_stats['min']
                    max_val = phase_stats['max']
                    std_val = phase_stats['std']
                except (KeyError, TypeError):
                    continue

                label_mean = 'Mean' if not stats_labels_added['mean'] else None
                ax.hlines(mean_val, start, end, color='black', linestyle='--',
                          linewidth=1.5, label=label_mean, zorder=11)
                stats_labels_added['mean'] = True

                label_min_max = 'Min/Max' if not stats_labels_added['min_max'] else None
                ax.hlines([min_val, max_val], start, end, color='red', linestyle=':',
                          linewidth=1.2, label=label_min_max, zorder=11)
                stats_labels_added['min_max'] = True

                label_std = 'Std Dev' if not stats_labels_added['std'] else None
                ax.add_patch(Rectangle(
                    (start, mean_val - std_val),
                    end - start,
                    2 * std_val,
                    color='black', alpha=0.1, zorder=5, label=label_std
                ))
                stats_labels_added['std'] = True

        # --- D. Finalize this subplot ---
        ax.set_ylabel(col)
        ax.legend(loc='upper left')
        ax.grid(True, alpha=0.4, linestyle='--')

        try:
            data_min = df_plot[col].min()
            data_max = df_plot[col].max()
            padding = (data_max - data_min) * 0.1
            ax.set_ylim(data_min - padding, data_max + padding)
        except Exception:
            pass

    # --- 6. Finalize, Save, and Show ---
    axes[-1].xaxis.set_major_formatter(DateFormatter('%H:%M:%S'))
    axes[-1].set_xlabel("Timestamp (HH:MM:SS)")

    legend_patches = [
        mpatches.Patch(color=c, alpha=0.4, label=f"{p} window")
        for p, c in phase_colors.items()
    ]
    fig.legend(
        handles=legend_patches,
        loc='upper center',
        bbox_to_anchor=(0.5, 0.98),
        ncol=len(phases),
        fontsize=10
    )

    fig.suptitle(f"EEG Trends on {plot_date} with Test Types", y=1.03, fontsize=16)

    try:
        plt.tight_layout(rect=[0, 0, 1, 0.95])

        if save_plot:
            plt.savefig(out_png_path, dpi=150, bbox_inches='tight')
            print(f"Saved plot → {out_png_path}")

        if show_plot:
            print("Displaying plot...")
            plt.show()

    except Exception as e:
        print(f"Error saving/showing plot: {e}")
    finally:
        if not show_plot:
            plt.close(fig)

# --- 5. Run the NEW visualization function ---
if not eeg_labeled_df.empty:

    # Define the columns we want to see
    cols_to_plot = ['Alpha(%)', 'Beta(%)', 'Theta(%)', 'Delta(%)']

    stats_df = calculate_eeg_statistics(eeg_labeled_df, cols_to_plot)

# stats_df

# --- Example Usage ---

# Suppress warnings
warnings.filterwarnings("ignore", message="The get_cmap function was deprecated")

# --- 5. Run the NEW visualization function ---
if not eeg_labeled_df.empty:

    # Define the columns we want to see
    cols_to_plot = ['Alpha(%)', 'Beta(%)', 'Theta(%)', 'Delta(%)']

    visualize_eeg_trends_advanced(
        eeg_labeled_df,
        columns_to_visualize=cols_to_plot,
        save_plot=False,
        show_plot=True, # Set to True to see the plot
        out_png_path="eeg_band_trends.png",
        show_statistics=True
    )

"""Calculate different metrics."""

import pandas as pd
import numpy as np
import warnings

def calculate_phase_ratios(stats_df):
    """
    Calculates specific phase-to-phase ratios for each brain wave
    using the 'mean' values from the statistics DataFrame.

    Metrics calculated:
    - Metric 1: preRun1 / postRun1
    - Metric 2: preRun2 / postRun2
    - Metric 3: (preRun1 + preRun2) / (postRun1 + postRun2)
    - Metric 4: (preRun1 + preRun2) / liveRun
    - Metric 5: liveRun / (postRun1 + postRun2)

    Args:
        stats_df (pd.DataFrame): The multi-index statistics DataFrame
                                 from calculate_eeg_statistics().

    Returns:
        pd.DataFrame: A DataFrame where the index is the brain wave
                      and the columns are the 5 calculated ratios.
    """

    # --- 1. Validation ---
    if stats_df is None or stats_df.empty:
        print("Ratio FAILED: Input stats_df is empty.")
        return pd.DataFrame()

    if 'mean' not in stats_df.columns.get_level_values(1):
        print("Ratio FAILED: 'mean' statistics not found in stats_df columns.")
        return pd.DataFrame()

    # --- 2. Extract Mean Stats ---
    # This creates a simple DataFrame where:
    # Index = testType (e.g., preRun1)
    # Columns = Brain Waves (e.g., alpha_%)
    try:
        mean_stats = stats_df.xs('mean', level=1, axis=1)
    except Exception as e:
        print(f"Ratio FAILED: Could not extract 'mean' stats. Error: {e}")
        return pd.DataFrame()

    # --- 3. Safely Get Phase Data ---
    # Create a helper function to safely get a phase's data (a Series)
    # or an empty Series if the phase is missing.
    def get_phase_series(phase_name):
        if phase_name in mean_stats.index:
            return mean_stats.loc[phase_name]
        else:
            print(f"Warning: Phase '{phase_name}' not found in stats. Ratios will be NaN.")
            # Return an empty series with the same index (brain waves)
            return pd.Series(dtype='float64', index=mean_stats.columns)

    pre1 = get_phase_series('preRun1')
    post1 = get_phase_series('postRun1')
    pre2 = get_phase_series('preRun2')
    post2 = get_phase_series('postRun2')
    live = get_phase_series('liveRun')

    # --- 4. Calculate Ratios ---
    # Pandas handles the element-wise division and addition,
    # aligning by the index (brain wave name) automatically.

    # Suppress divide-by-zero warnings (it will correctly show Inf or NaN)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)

        metric1 = pre1 / post1
        metric2 = pre2 / post2
        metric3 = (pre1 + pre2) / (post1 + post2)
        metric4 = (pre1 + pre2) / live
        metric5 = live / (post1 + post2)

    # --- 5. Assemble Final DataFrame ---
    ratios_df = pd.DataFrame({
        "pre1_div_post1": metric1,
        "pre2_div_post2": metric2,
        "pre_all_div_post_all": metric3,
        "pre_all_div_live": metric4,
        "live_div_post_all": metric5
    })

    # Rename index to be clear
    ratios_df.index.name = "Brain Wave"

    return ratios_df.round(3)

# --- 6. RUN THE NEW RATIO FUNCTION ---
if not stats_df.empty:
    print("\n\n--- Calculating Phase Ratios ---")
    ratios_df = calculate_phase_ratios(stats_df)

    print("\n\n--- FINAL RATIOS (ratios_df) ---")
    print(ratios_df)

import pandas as pd
import numpy as np
import warnings

def calculate_phase_ratios(stats_df):
    """
    Calculates phase-to-phase ratios for BOTH individual brain waves
    (e.g., alpha_%) AND complex brainwave ratios (e.g., Theta/Beta),
    returning them in a single, combined DataFrame.

    Args:
        stats_df (pd.DataFrame): The multi-index statistics DataFrame
                                 from calculate_eeg_statistics().

    Returns:
        pd.DataFrame: A TRANSPOSED DataFrame where:
                      - Index = Phase Ratio Metrics (e.g., "pre1_div_post1")
                      - Columns = All metrics (e.g., "alpha_%", "Theta/Beta", ...)
    """

    # --- 1. Validation ---
    if stats_df is None or stats_df.empty:
        print("Ratio FAILED: Input stats_df is empty.")
        return pd.DataFrame()

    if 'mean' not in stats_df.columns.get_level_values(1):
        print("Ratio FAILED: 'mean' statistics not found in stats_df columns.")
        return pd.DataFrame()

    # --- 2. Extract Mean Stats (Brain Waves per Phase) ---
    # Index = testType (e.g., preRun1), Columns = Brain Waves (e.g., alpha_%)
    try:
        mean_stats = stats_df.xs('mean', level=1, axis=1)
    except Exception as e:
        print(f"Ratio FAILED: Could not extract 'mean' stats. Error: {e}")
        return pd.DataFrame()

    # ================================================================
    # --- PART A: Calculate Ratios for Individual Brain Waves ---
    # ================================================================

    # Helper to get a Series of all brain waves for a specific phase
    def get_phase_series_simple(phase_name):
        if phase_name in mean_stats.index:
            return mean_stats.loc[phase_name]
        else:
            print(f"Warning: Phase '{phase_name}' not found. Simple ratios will be NaN.")
            return pd.Series(dtype='float64', index=mean_stats.columns)

    pre1_simple = get_phase_series_simple('preRun1')
    post1_simple = get_phase_series_simple('postRun1')
    pre2_simple = get_phase_series_simple('preRun2')
    post2_simple = get_phase_series_simple('postRun2')
    live_simple = get_phase_series_simple('liveRun')

    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)

        metric1_simple = pre1_simple / post1_simple
        metric2_simple = pre2_simple / post2_simple
        metric3_simple = (pre1_simple + pre2_simple) / (post1_simple + post2_simple)
        metric4_simple = (pre1_simple + pre2_simple) / live_simple
        metric5_simple = live_simple / (post1_simple + post2_simple)

    # Assemble DataFrame (Index = Brain Wave, Columns = Metric)
    simple_ratios_df = pd.DataFrame({
        "pre1_div_post1": metric1_simple,
        "pre2_div_post2": metric2_simple,
        "pre_all_div_post_all": metric3_simple,
        "pre_all_div_live": metric4_simple,
        "live_div_post_all": metric5_simple
    })

    # Transpose to get (Index = Metric, Columns = Brain Wave)
    simple_ratios_df_T = simple_ratios_df.transpose()


    # ================================================================
    # --- PART B: Calculate Ratios for Complex Brainwave Ratios ---
    # ================================================================

    # --- 3. Calculate Brainwave Ratios (for each phase) ---

    # Helper to find column name (e.g., 'alpha' in 'alpha_%')
    def find_col(name, all_cols):
        for col in all_cols:
            if name.lower() in col.lower():
                return col
        print(f"Warning: No brainwave column found for '{name}'. Ratios will be NaN.")
        return None

    # Find the column names in the DataFrame
    alpha_col = find_col('Alpha(%)', mean_stats.columns)
    beta_col = find_col('Beta(%)', mean_stats.columns)
    theta_col = find_col('Theta(%)', mean_stats.columns)
    delta_col = find_col('Delta(%)', mean_stats.columns)

    # Helper for safe division
    def safe_div(num_col_name, den_col_name):
        if num_col_name and den_col_name:
            denom = mean_stats[den_col_name].replace(0, np.nan)
            return mean_stats[num_col_name] / denom
        return np.nan

    # Create the intermediate DataFrame of brainwave ratios
    # Index = testType, Columns = new brainwave ratios
    brainwave_ratios_df = pd.DataFrame(index=mean_stats.index)
    brainwave_ratios_df['Theta/Beta'] = safe_div(theta_col, beta_col)
    brainwave_ratios_df['Delta/Beta'] = safe_div(delta_col, beta_col)
    brainwave_ratios_df['Delta/Alpha'] = safe_div(delta_col, alpha_col)
    brainwave_ratios_df['Alpha/Beta'] = safe_div(alpha_col, beta_col)

    if theta_col and alpha_col and beta_col:
        denom = mean_stats[beta_col].replace(0, np.nan)
        brainwave_ratios_df['(Theta+Alpha)/Beta'] = (mean_stats[theta_col] + mean_stats[alpha_col]) / denom
    else:
        brainwave_ratios_df['(Theta+Alpha)/Beta'] = np.nan

    brainwave_ratios_df = brainwave_ratios_df.dropna(axis=1, how='all')

    if brainwave_ratios_df.empty or brainwave_ratios_df.columns.empty:
        print("Warning: No complex brainwave ratios could be calculated. Returning simple ratios only.")
        return simple_ratios_df_T.round(3)

    # --- 4. Safely Get Phase Data (from brainwave_ratios_df) ---
    def get_phase_series_complex(phase_name):
        if phase_name in brainwave_ratios_df.index:
            return brainwave_ratios_df.loc[phase_name]
        else:
            print(f"Warning: Phase '{phase_name}' not found. Complex ratios will be NaN.")
            return pd.Series(dtype='float64', index=brainwave_ratios_df.columns)

    pre1_complex = get_phase_series_complex('preRun1')
    post1_complex = get_phase_series_complex('postRun1')
    pre2_complex = get_phase_series_complex('preRun2')
    post2_complex = get_phase_series_complex('postRun2')
    live_complex = get_phase_series_complex('liveRun')

    # --- 5. Calculate Phase Ratios (on the brainwave ratios) ---
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)

        metric1_complex = pre1_complex / post1_complex
        metric2_complex = pre2_complex / post2_complex
        metric3_complex = (pre1_complex + pre2_complex) / (post1_complex + post2_complex)
        metric4_complex = (pre1_complex + pre2_complex) / live_complex
        metric5_complex = live_complex / (post1_complex + post2_complex)

    # Assemble DataFrame (Index = Brainwave Ratio, Columns = Metric)
    complex_ratios_df = pd.DataFrame({
        "pre1_div_post1": metric1_complex,
        "pre2_div_post2": metric2_complex,
        "pre_all_div_post_all": metric3_complex,
        "pre_all_div_live": metric4_complex,
        "live_div_post_all": metric5_complex
    })

    # Transpose (Index = Metric, Columns = Brainwave Ratio)
    complex_ratios_df_T = complex_ratios_df.transpose()

    # ================================================================
    # --- PART C: Combine and Return ---
    # ================================================================

    # Join the two DataFrames side-by-side
    # Both have the same index (the 5 phase-ratio metrics)
    final_df = pd.concat([simple_ratios_df_T, complex_ratios_df_T], axis=1)

    final_df.index.name = "Phase_Ratio_Metric"

    return final_df.round(3)

if not stats_df.empty:
    print("\n\n--- Calculating Phase Ratios ---")
    ratios_df = calculate_phase_ratios(stats_df)

    print("\n\n--- FINAL RATIOS (ratios_df) ---")
    print(ratios_df)

"""### Handle Patient Statistics"""

import pandas as pd
from pathlib import Path

def get_needed_participants(file_path):
    """
    Loads the participant lookup CSV, filters it for rows where
    the 'Needed?' column is 'Y', and returns the 'Date' and 'KeyCode'
    for those rows.

    Args:
        file_path (str or Path): The file path to the
                                 'LIVE_participant_ID_name_lookup.csv' file.

    Returns:
        pd.DataFrame: A DataFrame containing 'Date' and 'KeyCode' for all
                      participants marked as 'Y'. Returns an empty
                      DataFrame on error or if no 'Y' rows are found.
    """

    file_path = Path(file_path)

    # --- 1. Read File ---
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return pd.DataFrame()
    except Exception as e:
        print(f"Error reading file: {e}")
        return pd.DataFrame()

    # --- 2. Validation ---
    required_cols = ['Date', 'KeyCode', 'Needed?']
    if not all(col in df.columns for col in required_cols):
        print(f"Error: The file must contain {required_cols} columns.")
        return pd.DataFrame()

    # --- 3. Filter Data ---

    # Handle potential NaN values in 'Needed?' by filling them with 'N'
    # This ensures they are not included in the 'Y' filter.
    df['Needed?'] = df['Needed?'].fillna('N')

    # Create the filter mask by stripping whitespace and converting to uppercase
    # to safely catch 'Y', ' Y', 'y', etc.
    mask = df['Needed?'].str.strip().str.upper() == 'Y'

    # Apply the mask and select the desired columns
    filtered_df = df.loc[mask, ['Date', 'KeyCode']].copy()

    # Reset the index for a clean DataFrame
    filtered_df = filtered_df.reset_index(drop=True)

    return filtered_df


# --- Example Usage ---

FILE_PATIENTS = "/content/drive/MyDrive/TamoTech/Synthia/LIVE_participant_ID_name_lookup.csv"

if Path(FILE_PATIENTS).exists():
    print(f"Loading and filtering file: {FILE_PATIENTS}\n")

    # Call the new function
    needed_data_df = get_needed_participants(FILE_PATIENTS)

    if not needed_data_df.empty:
        print(f"Found {len(needed_data_df)} participants marked as 'Needed'.")
        print("\n--- Extracted Data ---")
        print(needed_data_df)
    else:
        print("No participants marked as 'Y' were found.")
else:
    print(f"Example file '{FILE_PATIENTS}' not found. Cannot run example.")

def get_keycode_dict_by_date(participants_df):
    """
    Takes the filtered participant DataFrame and converts it into a
    dictionary where keys are formatted dates (DD_MM_YY) and
    values are lists of KeyCodes for that date.

    Args:
        participants_df (pd.DataFrame): The DataFrame from
                                        get_needed_participants(),
                                        must have 'Date' and 'KeyCode' cols.

    Returns:
        dict: A dictionary mapping formatted dates to lists of KeyCodes.
              Returns an empty dictionary on error.
    """

    # --- 1. Validation ---
    if participants_df.empty:
        print("Input DataFrame is empty. Returning empty dictionary.")
        return {}

    if 'Date' not in participants_df.columns or 'KeyCode' not in participants_df.columns:
        print("Error: DataFrame must have 'Date' and 'KeyCode' columns.")
        return {}

    # --- 2. Create a copy to avoid changing the original df ---
    df = participants_df.copy()

    try:
        # --- 3. Parse and Format Date ---

        # --- THE FIX IS HERE ---
        # Use format='mixed' to handle both 2-digit ('%y') and
        # 4-digit ('%Y') years automatically.
        df['datetime'] = pd.to_datetime(df['Date'], format='mixed')
        # --- END OF FIX ---

        # Create the new formatted date string
        # %d = Day (23), %m = Month (10), %y = Year (25)
        df['formatted_date'] = df['datetime'].dt.strftime('%d_%m_%y')

    except Exception as e:
        print(f"Error parsing date column: {e}")
        print("The 'Date' column appears to have inconsistent formats.")
        return {}

    # --- 4. Group, Aggregate, and Convert to Dict ---
    grouped = df.groupby('formatted_date')
    keycode_lists = grouped['KeyCode'].apply(list)
    date_keycode_map = keycode_lists.to_dict()

    return date_keycode_map

# 2. Call the new (second) function
print("\n\nConverting to dictionary...")
keycode_dict = get_keycode_dict_by_date(needed_data_df)

print("\n--- Final KeyCode Dictionary ---")
import json
# Pretty print the dictionary for clarity
print(json.dumps(keycode_dict, indent=2))

# --- NEW Function 3: As requested (Modified) ---

def get_full_paths_for_date(date_keycode_map, date_key, base_path):
    """
    Creates a list of FULL folder paths for a *specific* date key
    by combining the base path, the date key, and its corresponding KeyCodes.

    Converts KeyCodes from string/scientific notation to integer.

    Args:
        date_keycode_map (dict): The dictionary from get_keycode_dict_by_date()
                                 (e.g., {'23_10_25': ['5.90012E+12', ...]}).
        date_key (str): The specific date key you want to process
                        (e.g., '23_10_25').
        base_path (str or Path): The root folder path (e.g., '/path/to/data').

    Returns:
        list: A list of unique, full, absolute path strings
              (e.g., ['/path/to/data/23_10_25/5900123102510', ...]).
    """

    path_list = []

    # --- 1. Use Pathlib for robust path handling ---
    base_path = Path(base_path)

    # 2. Safely get the list of keycodes for the specified date
    keycode_list = date_keycode_map.get(date_key)

    # 3. Check if the date key was found
    if not keycode_list:
        print(f"Warning: Date key '{date_key}' not found in dictionary. Returning empty list.")
        return path_list

    # --- 4. Create the full date path *once* ---
    date_path = base_path / str(date_key)

    # 5. Iterate over each keycode in the list for that one date
    for keycode in keycode_list:
        try:
            # --- Convert KeyCode to INT ---
            keycode_float = float(keycode)
            keycode_int = int(keycode_float)

            # --- Create the final, full path ---
            final_path_str = str(date_path / str(keycode_int))

            if final_path_str not in path_list:
                path_list.append(final_path_str)

        except (ValueError, TypeError) as e:
            # This catches bad strings, None, NaN, etc.
            print(f"Warning: Could not convert KeyCode '{keycode}' to int. Skipping. Error: {e}")

    return path_list

# Define your main data folder
BASE_DATA_PATH = "/content/drive/MyDrive/TamoTech/Synthia"

# --- Specify the date you want to process ---
TARGET_DATE = "23_10_25"

print(f"\n\nGetting full paths for specific date: {TARGET_DATE}")

full_paths = get_full_paths_for_date(keycode_dict, TARGET_DATE, BASE_DATA_PATH)

print("\n--- Final List of Full Paths ---")
for path in full_paths:
    print(path)

"""## Extract All Patient's EEG Trends"""

# Define your main data folder
BASE_DATA_PATH = "/content/drive/MyDrive/TamoTech/Synthia"

# --- Specify the date you want to process ---
TARGET_DATE = "23_10_25"

FILE_PATIENTS = "/content/drive/MyDrive/TamoTech/Synthia/LIVE_participant_ID_name_lookup.csv"

# You need to define the weights based on your domain knowledge or requirements
consciousness_weights = {'CSI': 0.6, 'EMG': 0.3, 'SQI': 0.1, 'BS': 0.0} # Example weights, sum must be 1

# Analysis thresholds
analysis_thresholds = {
    'Delta(%)': 50,
    'Theta(%)': 30,
    'Beta(%)': 20,
    'Alpha(%)': 25
}

# Define the columns we want to see
cols_to_plot = ['Alpha(%)', 'Beta(%)', 'Theta(%)', 'Delta(%)']

if Path(FILE_PATIENTS).exists():
    print(f"Loading and filtering file: {FILE_PATIENTS}\n")

    # Get the details of valid Patient's key codes for each date
    needed_data_df = get_needed_participants(FILE_PATIENTS)

    print("\n\nConverting to dictionary...")
    keycode_dict = get_keycode_dict_by_date(needed_data_df)

    # get the paths with the selected date
    full_paths = get_full_paths_for_date(keycode_dict, TARGET_DATE, BASE_DATA_PATH)

else:
    # TODO: say the given path is invalid
    pass


# TODO: we need to get the patient ID by extracting the last part of the
# location in each path from the full_paths list
for patient_id in keycode_dict[TARGET_DATE]:

    eeg_file = find_files(main_folder, str(int(patient_id)), file_type="EEG")[0]
    hrv_file = find_files(main_folder, str(int(patient_id)), file_type="HRV")[0]
    print(eeg_file, hrv_file)
    print("---")

    # processing EEG data
    processed_eeg_df = process_eeg_data(eeg_file)

    # validation of EEG data
    eeg_validation_results = validate_eeg_data(processed_eeg_df)

    # # only if it is needed
    # plot_consciousness_metrics(processed_eeg_df, consciousness_weights)
    # plot_brain_wave_frequencies(processed_eeg_df)
    # plot_continuous_eeg_signal(processed_eeg_df)

    # # for combined metrics
    # plot_start_time = processed_eeg_df['Timestamp'].min() # Use minimum timestamp for start
    # plot_end_time = processed_eeg_df['Timestamp'].max() # Use maximum timestamp for end
    # plot_combined_metrics_eeg(processed_eeg_df, plot_start_time, plot_end_time)
    # plot_combined_metrics_eeg_heatmap_aligned(processed_eeg_df, plot_start_time, plot_end_time)

    # # calculate the consciousness score
    # if all(col in processed_eeg_df.columns for col in consciousness_weights.keys()):
    #     processed_eeg_df['Consciousness Score'] = (
    #         processed_eeg_df['CSI'] * consciousness_weights.get('CSI', 0) +
    #         processed_eeg_df['EMG'] * consciousness_weights.get('EMG', 0) +
    #         processed_eeg_df['SQI'] * consciousness_weights.get('SQI', 0) +
    #         processed_eeg_df['BS'] * consciousness_weights.get('BS', 0)
    #     )
    # else:
    #     print("Warning: Could not calculate 'Consciousness Score' for the main DataFrame.")

    # comprehensive_results = comprehensive_eeg_analysis(processed_eeg_df, analysis_thresholds)

    # loading HRV dataframe
    hrv_df = load_hrv_json_to_df(hrv_file)

    print("\nCalculating testType timings...")
    hrv_timings_df = get_testType_timings(hrv_df)

    # this is the validation for timing of both EEG and HRV data
    # if this is not, need to have the higher opinion
    is_valid = validate_timestamp_alignment(processed_eeg_df, hrv_df)
    print(f"Final Result: {is_valid}\n")

    # assign labelling of test types to the EEG dataframe
    eeg_labeled_df = assign_hrv_testTypes_to_eeg(processed_eeg_df, hrv_timings_df)

    # statistics of the EEG brain waves
    stats_df = calculate_eeg_statistics(eeg_labeled_df, cols_to_plot)

    # # visualise the EEG data with test types
    # visualize_eeg_trends_advanced(
    #     eeg_labeled_df,
    #     columns_to_visualize=cols_to_plot,
    #     save_plot=False,
    #     show_plot=True, # Set to True to see the plot
    #     out_png_path="eeg_band_trends.png",
    #     show_statistics=True
    # )

    print("\n\n--- Calculating Phase Ratios ---")
    ratios_df = calculate_phase_ratios(stats_df)

    display(ratios_df)

